{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Welcome to the first practice exercise: Data Inspection and Exploration**\n",
    "\n",
    "***Before you begin, ensure you have completed the \"Exercise Prerequisites\" tasks.***\n",
    "\n",
    "***Go through the \"Python - Pandas\" slides as well for a quicker understanding of the code to follow.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice exercise, you will go through the typical preprocessing steps that a dataset undergoes before it's fed to whatever ML algorithms to develop AI-based applications.\n",
    "\n",
    "These preprocessing steps are pretty much universal, unless you have a perfectly curated and calibrated dataset, which is rare, if not non-existent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the following two libraries:\n",
    "1. pandas - helps us in handling and preprocessing the data\n",
    "2. matplotli.pyplot - for visualizing the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # 'pd' is now an alias for the pandas module\n",
    "import matplotlib.pyplot as plt # 'plt' is now an alias for the matplotlib.pyplot module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the next step would be to load the dataset. The simplest way would be to download your dataset and put the file in the root directory. This exercise uses the \"Titanic\" dataset, and is already included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the path to the file. You can use either the full path or the relative path.\n",
    "file_path = 'titanic.csv' \n",
    "\n",
    "## Read the data from the file, which is in CSV format, into a pandas DataFrame (df).\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "## Print the first 5 rows of the DataFrame.\n",
    "print(df.head())\n",
    "\n",
    "## Print the first 5 rows of the DataFrame, but in a more readable format\n",
    "print(df.head().to_string())\n",
    "\n",
    "## Print the 'Name' column from the DataFrame.\n",
    "print(df['Name'])\n",
    "\n",
    "## Print multiple columns\n",
    "print(df[['Name', 'Age']])\n",
    "\n",
    "## Print the number of rows and columns in the DataFrame.\n",
    "print(df.shape) \n",
    "\n",
    "## Print the data types of the columns in the DataFrame.\n",
    "print(df.dtypes)\n",
    "\n",
    "## Print the summary statistics of the DataFrame.\n",
    "print(df.describe())\n",
    "\n",
    "## Print the number of missing values in each column of the DataFrame.\n",
    "print(df.isnull().sum())\n",
    "\n",
    "## Print the number of unique values in each column of the DataFrame.\n",
    "print(df.nunique())\n",
    "\n",
    "## Print the number of unique values from a specific column. Here, 'Embarked' column\n",
    "print(df['Embarked'].nunique())\n",
    "\n",
    "## TODO: Try other operations on the DataFrame. You will fined them in the \"Pandas\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another user-friendly way of loading the dataset. Assume you don't want to hardcode the path, and want the user to instead supply the path to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'input' function to take input from the user. This is equivalent to the 'Scanner' class in Java.\n",
    "file_path = input(\"Enter the path to your CSV file: \") \n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you proceed to handling the missing values in your dataset, ensure that every column is of right data type. For example, a numerical column is recorded as a String. Conversely, date-time column may be stored as a string. These should be handled before anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:    \n",
    "    \n",
    "    # 'pd.api.types.is_object_dtype' checks if the column is of object type (i.e., string or categorical)\n",
    "    if pd.api.types.is_object_dtype(df[col]):\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "            print(f\"Converted column '{col}' to numeric type\")\n",
    "        except ValueError:\n",
    "            # If conversion fails, it's likely a true categorical column\n",
    "            print(f\"Column '{col}' remains categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's handle missing values. \n",
    "\n",
    "Earlier, when you ran the code to find out columns with empty (NA) values, you would've come across handful of columns that had a lot of NAs. \n",
    "\n",
    "If there are too many NAs in a column, it's better to simply drop that column altogether, provided the said column is not important to your analysis. If it is, then there's not much you can do about it, except looking for a different dataset. The alternative of replacing excess number of NAs with mean/median/mode values can distort the dataset and adversely affect your analysis outcomes.\n",
    "\n",
    "But if there are few NAs, you may drop the entire corresponding row, or replace them with the applicable mean/median/mode values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the percentage of missing values in each column by iterating over the dataset\n",
    "for col in df.columns:\n",
    "    missing_percentage = df[col].isna().mean() * 100\n",
    "    print(f\"Checking '{col}' for missing values: {missing_percentage:.2f}% missing\")\n",
    "\n",
    "    # If more than 50% of values are missing, drop the column. \n",
    "    # Here, 50% is just an arbitraty threshold. It can be changed to any other value based on the use case.\n",
    "    if missing_percentage > 50:\n",
    "        print(f\"Dropping column '{col}' (because more than 50% missing)\")\n",
    "        df.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have handled columns with excess number of missing values, you can handle the rest by imputation, which is simply replacing the missing values with mean, median, or mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Impute missing values in each column by iterating over the dataset\n",
    "for col in df.columns:\n",
    "\n",
    "    # Impute numeric columns with mean or median, and categorical columns with mode\n",
    "    if pd.api.types.is_numeric_dtype(df[col]): # Check if the column is numeric\n",
    "        if missing_percentage > 0:\n",
    "            median_val = df[col].median()\n",
    "            print(f\"Imputing missing values in '{col}' with median: {median_val}\")\n",
    "            \n",
    "            # Replace missing values with the median of the column. \n",
    "            # Medians are safer than means, as they are less affected by outliers.\n",
    "            df[col].fillna(median_val, inplace=True) # 'inplace' parameter is used to modify the DataFrame directly, instead of creating a brand new DataFrame, which is the default behavior.\n",
    "    else:\n",
    "        if missing_percentage > 0:\n",
    "            mode_val = df[col].mode()[0]\n",
    "            print(f\"Imputing missing values in '{col}' with mode: {mode_val}\")\n",
    "            df[col].fillna(mode_val, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that NAs have been handled, there are still other issues that may affect the quality of the dataset. Obvious ones would be incorrectly formatted data type (commas in numbers), incorrect data (negative values, impossible values, etc.), or duplicates. Code to address every one of these issue is available in the \"Pandas\" slides. Use it to try the operations needed to further clean your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checking and Fixing Incorrect Data Types by\n",
    "# Firstly, Converting columns with commas in numbers to numeric type\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        try:\n",
    "            df[col] = df[col].str.replace(',', '').astype(float)\n",
    "            print(f\"Converted '{col}' to numerical data type.\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Handle Negative and Impossible Values\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    if (df[col] < 0).any():\n",
    "        print(f\" Seen negative values in '{col}'. making it NaN.\")\n",
    "        df.loc[df[col] < 0, col] = np.nan\n",
    "\n",
    "\n",
    "# solving duplicates issues\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows is: {duplicate_rows}\")\n",
    "\n",
    "if duplicate_rows > 0:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(\"Duplicated rows have been dropped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is not necessary for preparing your data, but it certainly helps you understand your data. The following code helps you classify every column as either nominal, ordinal, interval, or ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "\n",
    "    unique_vals = df[col].nunique()\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        # Numeric data with few unique values, treat as ordinal\n",
    "        if unique_vals < 10: # Arbitrary threshold for number of unique values. Can be changed based on the use case.\n",
    "            median_val = df[col].median()\n",
    "            print(f'{col}: Numeric Ordinal - Median: {median_val}')\n",
    "            #plot_boxplot(col, col)  # Plot a boxplot for ordinal numeric data\n",
    "            \n",
    "        else:\n",
    "            # Numeric data with many unique values, treat as continuous\n",
    "            mean_val = df[col].mean()\n",
    "            print(f'{col}: Numeric (Interval/Ratio) - Mean: {mean_val}')\n",
    "            #plot_histogram(col)  # Plot a histogram for continuous numeric data\n",
    "            \n",
    "    elif pd.api.types.is_object_dtype(df[col]):\n",
    "        # Non-numeric ordinal data (e.g., Low, Medium, High)\n",
    "        if unique_vals < 10:\n",
    "            mode_val = df[col].mode()[0]\n",
    "            print(f'{col}: Non-Numeric Ordinal - Mode: {mode_val}')\n",
    "            #plot_bar_chart(col)  # Plot a bar chart for ordinal non-numeric data\n",
    "            \n",
    "        else:\n",
    "            print(f'{col}: Non-Numeric Categorical (Too many unique values)')\n",
    "            \n",
    "    else:\n",
    "        print(f'{col}: Unrecognized type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know what type of data your dataset comprises, you can start to visualize them to gain some preliminary insights, which can help you in picking out the right variables (features) for ML algorithm training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start with boxplot. This plot is useful for plotting nominal/ordinal vs. interval/ratio types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following simply creates a list out of all the columns that are not numeric or have less than 10 unique values\n",
    "ordinal_or_nominal_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col]) or df[col].nunique() < 10]\n",
    "\n",
    "# The following creates a list of all the columns that are numeric\n",
    "numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "print(\"\\nOrdinal/Nominal columns available for X-axis:\")\n",
    "# 'enumerate' is used to get the index of the column along with the column name\n",
    "for i, col in enumerate(ordinal_or_nominal_cols):\n",
    "    print(f\"{i+1}: {col}\")\n",
    "\n",
    "print(\"\\nNumeric columns available for Y-axis:\")\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    print(f\"{i+1}: {col}\")\n",
    "\n",
    "# Ask the user to select the X-axis and Y-axis columns. \n",
    "# 'int' is used to convert the input to an integer, as 'input' returns a string by default.\n",
    "# '-1' is used to convert the user's choice to the correct index (since Python is 0-indexed)\n",
    "x_idx = int(input(\"Enter the number for the X-axis column: \")) - 1\n",
    "y_idx = int(input(\"Enter the number for the Y-axis column: \")) - 1\n",
    "\n",
    "# Check if the user's choice is within the valid range of column indices\n",
    "if 0 <= x_idx < len(ordinal_or_nominal_cols) and 0 <= y_idx < len(numeric_cols):\n",
    "    df.boxplot(column=numeric_cols[y_idx], by=ordinal_or_nominal_cols[x_idx])\n",
    "    plt.title(f'Box Plot of {numeric_cols[y_idx]} by {ordinal_or_nominal_cols[x_idx]}')\n",
    "    plt.suptitle('')  # Removes the default Pandas title\n",
    "    plt.xlabel(x_idx)\n",
    "    plt.ylabel(y_idx)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Invalid selection. Please select numbers corresponding to the column list.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is scatterplot, which is a plot between two interval/ratio variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNumeric columns available for scatterplot:\")\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    print(f\"{i+1}: {col}\")\n",
    "\n",
    "# here both the columns will be numerical\n",
    "x_idx = int(input(\"Enter the number for the X-axis column: \")) - 1\n",
    "y_idx = int(input(\"Enter the number for the Y-axis column: \")) - 1\n",
    "\n",
    "if 0 <= x_idx < len(numeric_cols) and 0 <= y_idx < len(numeric_cols):\n",
    "    df.plot.scatter(x=numeric_cols[x_idx], y=numeric_cols[y_idx], title=f'Scatter Plot of {numeric_cols[x_idx]} vs {numeric_cols[y_idx]}')\n",
    "    plt.xlabel(numeric_cols[x_idx])\n",
    "    plt.ylabel(numeric_cols[y_idx])\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Invalid selection. Please select numbers corresponding to the column list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is bar chart. This visualization is meant for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col]) or df[col].nunique() < 10]\n",
    "\n",
    "for i, col in enumerate(ordinal_cols):\n",
    "    print(f\"{i+1}: {col}\")\n",
    "\n",
    "# here the user enters value only for the X-axis, as Y-axis will be frequency by default. \n",
    "num_col = int(input(\"Enter the number for the X-axis column: \")) - 1\n",
    "\n",
    "df[ordinal_cols[num_col]].value_counts().plot(kind='bar', title=f'Bar Chart of {ordinal_cols[num_col]}')\n",
    "plt.xlabel(ordinal_cols[num_col])\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, histogram. This visualization is helpful in figuring out the distribution of your numerical (interval or ratio) data, whether it's normally distributed or skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(numeric_cols):\n",
    "    print(f\"{i+1}: {col}\")\n",
    "\n",
    "# here, the user selects only one column, and it will be a numerical column\n",
    "num_col = int(input(\"Enter the number for the X-axis column: \")) - 1\n",
    "\n",
    "df[numeric_cols[num_col]].plot(kind='hist', title=f'Histogram of {numeric_cols[num_col]}')\n",
    "plt.xlabel(numeric_cols[num_col])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
